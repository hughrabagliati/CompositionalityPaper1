a = data.frame(d1 = c(1,2,1),d2 = c(2,1,1),d3 = c(1,1,1))
a
which(a==2)
a[which(a==2)]
a[which(a==2),]
which(a==2, arr.ind = T)
a[which(a==2, arr.ind = T),]
a[arrayInd(a==2),]
a[arrayInd(a==2)]
arrayInd(a==2)
arrayInd(a,2)
which(a==2, arr.ind = T)
which(a>2, arr.ind = T)
which(a12, arr.ind = T)
which(a>1, arr.ind = T)
which(a>1, arr.ind = T) -> k
a[k]
a[!k]
Data Analysis Scripts for analysis of population alone#
			calculate.statistics.within = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength + (1+Cond*Strength|Subj) , data = x)))[2:4,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
#
			}#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.within = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.within(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
			StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
			stats.sample = calculate.statistics.within(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			#do the resampling#
			Resamples_N = 2500 # Number of samples (it will take 5-10 minutes to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample.within(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			Within.Resamples = list(resample1.large.cluster,resample2.large.cluster,resample3.large.cluster, paste(Resamples_N, "Samples on date ", date()))#
			Within.Clusters = list(sample.cluster1,sample.cluster2,sample.cluster3)#
			save(Within.Clusters,within.Resamples,file =paste("WithinStats",substr(date(),5,7),substr(date(),22,23),".RDATA", sep = ""))
library(doBy)
Data Analysis Scripts for analysis of population alone#
			calculate.statistics.within = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength + (1+Cond*Strength|Subj) , data = x)))[2:4,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
#
			}#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.within = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.within(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
			StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
			stats.sample = calculate.statistics.within(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			#do the resampling#
			Resamples_N = 2500 # Number of samples (it will take 5-10 minutes to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample.within(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			Within.Resamples = list(resample1.large.cluster,resample2.large.cluster,resample3.large.cluster, paste(Resamples_N, "Samples on date ", date()))#
			Within.Clusters = list(sample.cluster1,sample.cluster2,sample.cluster3)#
			save(Within.Clusters,within.Resamples,file =paste("WithinStats",substr(date(),5,7),substr(date(),22,23),".RDATA", sep = ""))
sample.cluster3
sample.cluster1
Data Analysis Scripts for analysis of population alone#
			calculate.statistics.within = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength + (1+Cond*Strength|Subj) , data = x)))[2:4,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
#
			}#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.within = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.within(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
			StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
			stats.sample = calculate.statistics.within(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			#do the resampling#
			Resamples_N = 250 # Number of samples (it will take 5-10 minutes to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample.within(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			Within.Resamples = list(resample1.large.cluster,resample2.large.cluster,resample3.large.cluster, paste(Resamples_N, "Samples on date ", date()))#
			Within.Clusters = list(sample.cluster1,sample.cluster2,sample.cluster3)#
			save(Within.Clusters,within.Resamples,file =paste("WithinStats",substr(date(),5,7),substr(date(),22,23),".RDATA", sep = ""))
library(lme4)
summary(lmer(Score~Cond*Strength*Pop +(1+Cond*Strength|Subj) , data = subset(ET, Time == 400)))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond*Strength|Subj) , data = subset(ET, Time == 500)))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond*Strength|Subj) , data = subset(ET, Time == 900)))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 900)))
summary(subset(ET, Time == 900)
)
summary(subset(ET, Time == 800))
ET$Score <- ifelse(ET$Score <= -3.66, 0,1)
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 900), family = "binomial"))
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond*Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)
stats.sample
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)
stats.sample
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond*Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)#
			#do the resampling#
			Resamples_N = 25 # Number of samples (it will take a long time to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample4.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample5.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample6.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample7.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			recluster4 = find.clusters(stats.resample[[4]]$pval,stats.resample[[4]]$Stat,stats.resample[[4]]$Time,cluster.size = 1)#
			recluster5 = find.clusters(stats.resample[[5]]$pval,stats.resample[[5]]$Stat,stats.resample[[5]]$Time,cluster.size = 1)#
			recluster6 = find.clusters(stats.resample[[6]]$pval,stats.resample[[6]]$Stat,stats.resample[[6]]$Time,cluster.size = 1)#
			recluster7 = find.clusters(stats.resample[[7]]$pval,stats.resample[[7]]$Stat,stats.resample[[7]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
			resample4.max = ifelse(is.numeric(recluster4$cluster.stat),max(recluster4$cluster.stat),0)#
			resample5.max = ifelse(is.numeric(recluster5$cluster.stat),max(recluster5$cluster.stat),0)#
			resample6.max = ifelse(is.numeric(recluster6$cluster.stat),max(recluster6$cluster.stat),0)#
			resample7.max = ifelse(is.numeric(recluster7$cluster.stat),max(recluster7$cluster.stat),0)#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
		#	print(recluster1[recluster1$cluster.stat == max(recluster1$cluster.stat),]$start)#
		#	print(recluster3[recluster3$cluster.stat == max(recluster3$cluster.stat),]$start )#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			print(resample3.max)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			resample4.large.cluster[i] = ifelse(resample4.max>0,resample3.max,0)#
			resample5.large.cluster[i] = ifelse(resample5.max>0,resample3.max,0)#
			resample6.large.cluster[i] = ifelse(resample6.max>0,resample3.max,0)#
			resample7.large.cluster[i] = ifelse(resample7.max>0,resample3.max,0)#
#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			sample.cluster4 = pval(sample.cluster4,resample4.large.cluster)#
			sample.cluster5 = pval(sample.cluster5,resample5.large.cluster)#
			sample.cluster6 = pval(sample.cluster6,resample6.large.cluster)#
			sample.cluster7 = pval(sample.cluster7,resample7.large.cluster)
library(doBy)
Resamples_N = 25 # Number of samples (it will take a long time to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample4.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample5.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample6.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample7.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			recluster4 = find.clusters(stats.resample[[4]]$pval,stats.resample[[4]]$Stat,stats.resample[[4]]$Time,cluster.size = 1)#
			recluster5 = find.clusters(stats.resample[[5]]$pval,stats.resample[[5]]$Stat,stats.resample[[5]]$Time,cluster.size = 1)#
			recluster6 = find.clusters(stats.resample[[6]]$pval,stats.resample[[6]]$Stat,stats.resample[[6]]$Time,cluster.size = 1)#
			recluster7 = find.clusters(stats.resample[[7]]$pval,stats.resample[[7]]$Stat,stats.resample[[7]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
			resample4.max = ifelse(is.numeric(recluster4$cluster.stat),max(recluster4$cluster.stat),0)#
			resample5.max = ifelse(is.numeric(recluster5$cluster.stat),max(recluster5$cluster.stat),0)#
			resample6.max = ifelse(is.numeric(recluster6$cluster.stat),max(recluster6$cluster.stat),0)#
			resample7.max = ifelse(is.numeric(recluster7$cluster.stat),max(recluster7$cluster.stat),0)#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
		#	print(recluster1[recluster1$cluster.stat == max(recluster1$cluster.stat),]$start)#
		#	print(recluster3[recluster3$cluster.stat == max(recluster3$cluster.stat),]$start )#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			print(resample3.max)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			resample4.large.cluster[i] = ifelse(resample4.max>0,resample3.max,0)#
			resample5.large.cluster[i] = ifelse(resample5.max>0,resample3.max,0)#
			resample6.large.cluster[i] = ifelse(resample6.max>0,resample3.max,0)#
			resample7.large.cluster[i] = ifelse(resample7.max>0,resample3.max,0)#
#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			sample.cluster4 = pval(sample.cluster4,resample4.large.cluster)#
			sample.cluster5 = pval(sample.cluster5,resample5.large.cluster)#
			sample.cluster6 = pval(sample.cluster6,resample6.large.cluster)#
			sample.cluster7 = pval(sample.cluster7,resample7.large.cluster)
sample.cluster7
sample.cluster4
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
Resamples_N = 25 # Number of samples (it will take a long time to do 1000, so set this to 10 your first time to make sure the script works)#
			resample1.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample2.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample3.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample4.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample5.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample6.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			resample7.large.cluster = rep(NA,Resamples_N) #Preallocate your resample datafile #
			for (i in 1:Resamples_N){#
				print(i)#
			stats.resample = resample(ET,StartTime,EndTime,StepSize)#
			recluster1 = find.clusters(stats.resample[[1]]$pval,stats.resample[[1]]$Stat,stats.resample[[1]]$Time,cluster.size = 1)#
			recluster2 = find.clusters(stats.resample[[2]]$pval,stats.resample[[2]]$Stat,stats.resample[[2]]$Time,cluster.size = 1)#
			recluster3 = find.clusters(stats.resample[[3]]$pval,stats.resample[[3]]$Stat,stats.resample[[3]]$Time,cluster.size = 1)#
			recluster4 = find.clusters(stats.resample[[4]]$pval,stats.resample[[4]]$Stat,stats.resample[[4]]$Time,cluster.size = 1)#
			recluster5 = find.clusters(stats.resample[[5]]$pval,stats.resample[[5]]$Stat,stats.resample[[5]]$Time,cluster.size = 1)#
			recluster6 = find.clusters(stats.resample[[6]]$pval,stats.resample[[6]]$Stat,stats.resample[[6]]$Time,cluster.size = 1)#
			recluster7 = find.clusters(stats.resample[[7]]$pval,stats.resample[[7]]$Stat,stats.resample[[7]]$Time,cluster.size = 1)#
			resample1.max = ifelse(is.numeric(recluster1$cluster.stat),max(recluster1$cluster.stat),0)#
			resample2.max = ifelse(is.numeric(recluster2$cluster.stat),max(recluster2$cluster.stat),0)#
			resample3.max = ifelse(is.numeric(recluster3$cluster.stat),max(recluster3$cluster.stat),0)#
			resample4.max = ifelse(is.numeric(recluster4$cluster.stat),max(recluster4$cluster.stat),0)#
			resample5.max = ifelse(is.numeric(recluster5$cluster.stat),max(recluster5$cluster.stat),0)#
			resample6.max = ifelse(is.numeric(recluster6$cluster.stat),max(recluster6$cluster.stat),0)#
			resample7.max = ifelse(is.numeric(recluster7$cluster.stat),max(recluster7$cluster.stat),0)#
			 #Find the largest cluster (this is what you will be comparing against - is your cluster > 95% of largest clusters)#
			print(resample1.max)#
			print(resample3.max)#
		#	print(recluster1[recluster1$cluster.stat == max(recluster1$cluster.stat),]$start)#
		#	print(recluster3[recluster3$cluster.stat == max(recluster3$cluster.stat),]$start )#
			resample1.large.cluster[i] = ifelse(resample1.max>0,resample1.max,0)#
			resample2.large.cluster[i] = ifelse(resample2.max>0,resample2.max,0)#
			print(resample3.max)#
			resample3.large.cluster[i] = ifelse(resample3.max>0,resample3.max,0)#
			resample4.large.cluster[i] = ifelse(resample4.max>0,resample3.max,0)#
			resample5.large.cluster[i] = ifelse(resample5.max>0,resample3.max,0)#
			resample6.large.cluster[i] = ifelse(resample6.max>0,resample3.max,0)#
			resample7.large.cluster[i] = ifelse(resample7.max>0,resample3.max,0)#
#
			}#
#
			sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			sample.cluster4 = pval(sample.cluster4,resample4.large.cluster)#
			sample.cluster5 = pval(sample.cluster5,resample5.large.cluster)#
			sample.cluster6 = pval(sample.cluster6,resample6.large.cluster)#
			sample.cluster7 = pval(sample.cluster7,resample7.large.cluster)
sample.cluster1
sample.cluster4
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
summary(ET)
ET$Score <- ifelse(ET$Score <= -3.66, 0,1)
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
warnings()
summary(ET)
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 900), family = "binomial"))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 100), family = "binomial"))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 200), family = "binomial"))
summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 1400), family = "binomial"))
summary(glmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = subset(ET, Time == 1400), family = "binomial"))
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(glmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x, family = "binomial")))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
sample.cluster1 = pval(sample.cluster1,resample1.large.cluster)#
			sample.cluster2 = pval(sample.cluster2,resample2.large.cluster)#
			sample.cluster3 = pval(sample.cluster3,resample3.large.cluster)#
			sample.cluster4 = pval(sample.cluster4,resample4.large.cluster)#
			sample.cluster5 = pval(sample.cluster5,resample5.large.cluster)#
			sample.cluster6 = pval(sample.cluster6,resample6.large.cluster)#
			sample.cluster7 = pval(sample.cluster7,resample7.large.cluster)
sample.cluster1
sample.cluster3
sample.cluster4
Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample = function(data_file,StartTime,EndTime,StepSize){#
#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				shuf.fnc <- function(x){#
				NewCond <- x$Cond[sample.int(length(x$Cond))]#
				NewStrength <- x$Strength[sample.int(length(x$Strength))]#
				return(data.frame(x,NewCond,NewStrength))#
					}#
				Trials = ddply(Trials, .(Subj), shuf.fnc)#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				contrasts(data_file$Pop)[2] <- 1#
				contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}#
# Cluster finding function and pval finding function#
			# Cluster finding script (written by Jon Brennan )#
			find.clusters <- function(pval.vector, tval.vector, latencies, alpha=.05, cluster.size=5,signed =FALSE, sign = "pos") {#
				binary.stat <- as.numeric(pval.vector < alpha)#
				tval.stat <- rep(0,length(binary.stat))#
				tval.stat[2:length(binary.stat)] = ((tval.vector[2:length(binary.stat)] * tval.vector[1:(length(binary.stat)-1)]) <0)#
				cluster.start <- c()#
				cluster.end <- c()#
				cluster.stat <- c()#
				in.cluster <- 0#
				found.clusters <- 0#
				new.end <- 0#
				new.start <- 0#
				end.cluster <- 0#
				for (n in 1:length(binary.stat)) {#
					if (signed == FALSE){#
						if (in.cluster == 0 && binary.stat[n] == 1 ) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
					if (sign == "pos"){#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] >= 0) {#
							new.start = n#
							in.cluster = 1#
							}#
					}else#
						if (in.cluster == 0 && binary.stat[n] == 1 && tval.vector[n] <= 0) {#
							new.start = n#
							in.cluster = 1#
								}#
					if (in.cluster == 1 && binary.stat[n] == 0) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}#
					if (in.cluster == 1 && tval.stat[n] == 1) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 0			#
						}				#
					# in case we reach the end and we are still "in" a cluster...#
					if (in.cluster == 1 && binary.stat[n] == 1 && n == length(binary.stat)) {#
						new.end = n#
						end.cluster = 1#
						in.cluster = 1#
						}		#
					if (end.cluster) {	#
						if ((new.end - new.start) >= cluster.size) {#
							found.clusters <- found.clusters + 1#
							cluster.start<- c(cluster.start, latencies[new.start])#
							cluster.end <- c(cluster.end, latencies[new.end])#
							cluster.stat <- c(cluster.stat, sum(abs(tval.vector[new.start:(new.end-1)])))#
							}#
						end.cluster = 0#
						}#
					}#
				cluster.out <- data.frame(start = cluster.start, end = cluster.end, cluster.stat = cluster.stat)#
				return(cluster.out)	#
				}#
			# Script for assigning pvals to clusters	#
			pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics = function(data_file,StartTime,EndTime,StepSize){#
			#Load the relevant libraries#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)#
			#Start the multicore machine!#
#
			#Split the data frame into a list for each timepoint#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,]$Time)#
			#Parallel application of lmer and coefficient extraction, outputs another list as above#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength*Pop +(1+Cond+Strength|Subj) , data = x)))[2:8,1:3]) -> a.co#
			# Evaluate the list to see whether t value meets a threshold (hard coded)#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			# plyr that list into a dataframe#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","PopControl","CondU:Strengthweak","CondU:PopControl","Strengthweak:PopControl", "CondU:Strengthweak:PopControl"))#
			#Split that dataframe into a list based on the regression coefficients#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
# What analysis will we be doing at each time point?#
#
			calculate.statistics.nopop = function(data_file,StartTime,EndTime,StepSize){#
			require("lme4") #
			require(multicore)#
			require(doMC)#
			require(foreach)#
			require(plyr)			#
			a = split(data_file[data_file$Time >= StartTime & data_file$Time <= EndTime,],data_file[data_file$Time >= 0 & data_file$Time <= 1500,]$Time)#
			mclapply(a,function(x) coef(summary(lmer(Score~Cond*Strength +(1+Cond+Strength|Subj) + (1+Cond+Strength|Trial), data = x)))[2:4,1:3]) -> a.co#
			lapply(a.co,function(x) data.frame(x,pv = ifelse(abs(x[,3]) > 1.6,0.04,0.06), co = rownames(x))) -> k#
			k.df = ldply(k)#
			k.df$.id = as.numeric(k.df$.id)#
			colnames(k.df) <- c("Time", "Estimate","SE", "Stat", "pval","Coef")#
			k.df$Coef = ordered(k.df$Coef,levels = c("CondU","Strengthweak","CondU:Strengthweak"))#
			stats.sample = split(k.df,k.df$Coef)#
			return(stats.sample)#
			}#
pval = function(sample.cluster,resample.large.cluster){#
			sample.cluster$pval = 1#
			for (i in 1:length(sample.cluster$cluster.stat)){#
				print(i)#
				sample.cluster$pval[i] = 1 - sum(as.numeric(sample.cluster$cluster.stat[i]>resample.large.cluster)/Resamples_N)#
					}#
					return(sample.cluster)#
			}#
#
# Data Analysis Scripts#
#
# Function to shuffle condition labels assuming that we hold groups of trials constant. i.e., does the same as shuffling averaged data.#
			# Function to shuffle the condition label then compute the same stats as above#
			# This does the resampling on your data.#
			resample.nopop = function(data_file,StartTime,EndTime,StepSize){#
				summaryBy(Trial~Subj+Trial+Cond+Strength+Pop, data = data_file ) -> Trials#
				Trials$Order = NA#
                                Trials$NewCond = Trials$Cond#
                                Trials$NewStrength = Trials$Strength#
                                Trials$NewPop = Trials$Pop#
			for (i in unique(Trials$Subj)){#
				Trials[Trials$Subj == i,]$Order = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewCond = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Cond#
                                #Trials[Trials$Subj == i,]$NewOrder = sample.int(length(Trials[Trials$Subj == i,]$Cond),length(Trials[Trials$Subj == i,]$Cond))	#
				Trials[Trials$Subj == i,]$NewStrength = Trials[ order(Trials[Trials$Subj == i,]$Order),]$Strength                                #
			}#
				for (i in unique(data_file$Subj)){#
					data_file_S = data_file[data_file$Subj == i,]#
                                        data_file_S.prac = data_file_S#
					for (j in unique(data_file_S.prac$Trial)){#
                                           data_file_S[ data_file_S.prac$Trial == j,]$Cond =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewCond),length(data_file_S[ data_file_S.prac$Trial == j,]$Cond))#
                                          data_file_S[ data_file_S.prac$Trial == j,]$Strength =  rep(as.character(Trials[Trials$Subj == i & Trials$Trial == j,]$NewStrength),length(data_file_S[ data_file_S.prac$Trial == j,]$Strength))#
                                        }#
					data_file[data_file$Subj == i,] = data_file_S#
                                      }#
                                Trials <- summaryBy(Subj~Subj+Pop, data = Trials)#
                                Trials$Order = NA#
                                Trials$Order = sample.int(length(Trials$Order),length(Trials$Order))#
                                Trials$Pop = Trials[order(Trials$Order),]$Pop#
                                #print(Trials$Pop)#
#
                                for (i in unique(data_file$Subj)){ #
                                data_file[data_file$Subj == i,]$Pop = Trials[Trials$Subj == i,]$Pop#
                              }#
				data_file$Cond = as.factor(data_file$Cond)#
				data_file$Strength = as.factor(data_file$Strength)#
                                data_file$Pop = as.factor(data_file$Pop)#
				contrasts(data_file$Cond)[1] <- -1#
				contrasts(data_file$Cond)[2] <- 1#
				contrasts(data_file$Strength)[2] <- 1#
				contrasts(data_file$Strength)[1] <- -1#
				#contrasts(data_file$Pop)[2] <- 1#
				#contrasts(data_file$Pop)[1] <- -1#
			#	print(data_file[data_file$Subj == i,]$Pop[1:30])#
                        #       print(data_file[data_file$Subj == i,]$Trial[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Cond[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Strength[seq(300,1000, by = 25)])#
                       #        print(data_file[data_file$Subj == i,]$Score[seq(300,1000, by = 25)])#
				stats.resample = calculate.statistics.nopop(data_file,StartTime,EndTime,StepSize)#
				return(stats.resample)#
				}
StartTime = 0#
			EndTime = 1500#
			StepSize = 100#
			# Get the relevant stats for your data file.#
			registerDoMC();#
#
			stats.sample = calculate.statistics(ET,StartTime,EndTime,StepSize)#
			sample.cluster1 = find.clusters(stats.sample[[1]]$pval,stats.sample[[1]]$Stat,stats.sample[[1]]$Time,cluster.size = 1)#
			sample.cluster2 = find.clusters(stats.sample[[2]]$pval,stats.sample[[2]]$Stat,stats.sample[[2]]$Time,cluster.size = 1)#
			sample.cluster3 = find.clusters(stats.sample[[3]]$pval,stats.sample[[3]]$Stat,stats.sample[[3]]$Time,cluster.size = 1)#
			sample.cluster4 = find.clusters(stats.sample[[4]]$pval,stats.sample[[4]]$Stat,stats.sample[[4]]$Time,cluster.size = 1)#
			sample.cluster5 = find.clusters(stats.sample[[5]]$pval,stats.sample[[5]]$Stat,stats.sample[[5]]$Time,cluster.size = 1)#
			sample.cluster6 = find.clusters(stats.sample[[6]]$pval,stats.sample[[6]]$Stat,stats.sample[[6]]$Time,cluster.size = 1)#
			sample.cluster7 = find.clusters(stats.sample[[7]]$pval,stats.sample[[7]]$Stat,stats.sample[[7]]$Time,cluster.size = 1)
sample.cluster4
stats.sample[[4]]
stats.sample[[1]]
dat<- data.frame(t=seq(0, 2*pi, by=0.1) )#
 xhrt <- function(t) 16*sin(t)^3#
 yhrt <- function(t) 13*cos(t)-5*cos(2*t)-2*cos(3*t)-cos(4*t)#
 dat$y=yhrt(dat$t)#
 dat$x=xhrt(dat$t)#
 with(dat, plot(x,y, type="l"))
with(dat, polygon(x,y, col="hotpink"))
points(c(10,-10, -15, 15), c(-10, -10, 10, 10), pch=169, font=5)
#values used for longdiff#
# s_n<-24 #how many subjects#
# switch_baseline<-.25 #probability of switching to target at baseline#
# switch_on<-.5 #probability of switching to target when effect is in effect#
# switch_off<-.1 #probability of switching off target when effect is in effect#
# items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
# bins_n<-20 #how many bins?#
# first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
# time_diff<-2.5 #how many bins earlier is there an effect in condition 1 than condition 2?#
# noise<-2 #trial noise sd#
# sub_speeds<-rnorm(s_n,0,1) #how much faster or slower than typical is the subject?#
# sub_s_sd<-1 #sd for sub speed error#
# sub_effects<-rnorm(s_n,0,1.5) #degree to which effect size is larger or smaller for that subject#
# sub_e_sd<-1 #sd for sub effect error#
# item_effects<-rnorm(items_n,0,.5) #degree to which effect size is larger or smaller than typical for that item#
# item_sd<-1 #sd for item effect error#
#
#values used for shortdiff#
s_n<-24 #how many subjects#
switch_baseline<-.25 #probability of switching to target at baseline#
switch_on<-.9 #probability of switching to target when effect is in effect#
switch_off<-.2 #probability of switching off target when effect is in effect#
items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
bins_n<-20 #how many bins?#
first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
time_diff<-0 #how many bins earlier is there an effect in condition 1 than condition 2?#
noise<-.25 #trial noise sd#
sub_speeds<-rnorm(s_n,0,.25) #how much faster or slower than typical is the subject?#
sub_s_sd<-.5 #sd for sub speed error#
sub_effects<-rnorm(s_n,0,2) #degree to which effect size is larger or smaller for that subject#
sub_e_sd<-.25 #sd for sub effect error#
item_effects<-rnorm(items_n,0,2) #degree to which effect size is larger or smaller than typical for that item#
item_sd<-.25 #sd for item effect error
#what's alpha?#
1-.95^(1/bins_n)#
#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}
summary(mydata)
#what's alpha?#
1-.95^(1/bins_n)#
#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}
summary(mydata)
#what's alpha?#
1-.95^(1/bins_n)#
#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}
createdata <- function(){#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)+time_diff+rnorm(1,sub_effects[sub],sub_e_sd)+rnorm(1,item_effects[item],item_sd)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}#
mydata<-mydata[is.na(mydata$subject)==FALSE,]#
}
createdata <- function(){#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)+time_diff+rnorm(1,sub_effects[sub],sub_e_sd)+rnorm(1,item_effects[item],item_sd)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}#
return(mydata<-mydata[is.na(mydata$subject)==FALSE,])#
}
a<- createdata()
summary(a)
#analyze with mixed effects model#
library(lme4)#
get_stats = function(thedata){#
	#assumes data is in the format of the actual data above. Easy to adjust for other datasets.#
	#returns a dataframe, where each row is c(bin#, z-score, p-value)#
	sigs_me<-data.frame(bin=c(1:bins_n),z=rep(0,bins_n),p=rep(0,bins_n))#
	for (bin in c(1:bins_n)){#
		#since our subjects and items aren't different from one another, no need for maximal random effects#
		m<-glmer(fixtarg~condition+(1|subject)+(1|item),data=thedata[thedata$bin==bin,],family="binomial") #
		sigs_me[sigs_me$bin==bin,]<-c(bin,coef(summary(m))[2,3],coef(summary(m))[2,4])#
	}#
	return(sigs_me)#
}
threshold=1.5 #cutoff (t value) for inclusion in a cluster. Lower thresholds get longer, weaker clusters. Choice doesn't effect on false positive rate ... unless you keep trying different thresholds until something "works"#
#
#define some useful functions#
cluster = function(somedata,cutoff){#
	#somedata is a vector of test statistics (not p-values)#
	#cutoff is a threshold for for those statistics. Anything below threshold won't be considered part of a cluster#
	#returns a vector of the same length, where 0 marks a value that is not part of a cluster, and counting numbers mark clusters. #
	#Every value with same # is in same cluster.#
	somedata<-abs(somedata) #will cause problems if there is a sudden, significant switch in sign. Unlikely in practice for eyetracking data.#
	clusters=c()#
	current_cluster=0#
	in_cluster=FALSE#
	for (i in c(1:length(somedata))){#
		if (somedata[i]>cutoff){#
			if (in_cluster){#
				clusters<-c(clusters,current_cluster)#
			}else{#
				in_cluster=TRUE#
				current_cluster<-current_cluster+1#
				clusters<-c(clusters,current_cluster)#
			}#
		}else{#
			clusters<-c(clusters,0)	#
			in_cluster=FALSE#
		}#
	}#
	return(clusters)#
}#
#
score_clusters = function(somedata,someclusters){#
	#somedata is a vector of test statistics (not p-values)#
	#someclusters is the output of cluster()#
	#returns dataframe with the size (sum of test stats) of each cluster#
	c<-data.frame(stat=somedata,cluster=someclusters)#
	scores<-c()#
	for (cluster in c(1:max(someclusters))){#
		scores<-c(scores,sum(abs(c$stat[c$cluster==cluster])))#
	}#
	return(data.frame(cluster=c(1:length(scores)),score=scores))	#
}#
#
resample_data = function(thedata){#
	#This function assumes data is structured like the data above, but should be easy to adapt.#
	##
	#The design of this study is fully crossed (every subject gets every item in every condition)#
	#So all we need to do is shuffle the condition codes for each item for each subject. #
	#If there were also a between-subjects condition, we would have to shuffle subjects between conditions. Etc.#
	#Note: Do NOT shuffle bins and do NOT shuffle condition codes independently for each bin. #
	#That would assume the data in each bin of a trial is independent of the data in the other bins, which it is not.#
	for (sub in c(1:s_n)){#
		for (item in c(1:items_n)){#
			#random coin flip. If heads, switch condition codes. If tails, don't.#
			#if (rbinom(1,1,.5)){#
				#heads. switch codes.#
				# I'm not sure that your analysis does a random partition, Josh. Rather, all the trials that were previously #
				# in one bin, are now assigned to another.#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==1]<-3#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==2]<-1#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==3]<-2#
				#Making the partition random#
				new_cond <- sample(c(rep(c(1,2))),items_n, replace = TRUE)#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==1] <- 3#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==2] <- 4#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==3]<- new_cond[item]#
				#thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==4]<- new_cond[item+items_n]#
				thedata$condition[thedata$subject==sub & thedata$item==item & thedata$condition==4] <- ifelse(new_cond[item] ==1,2,1)#
			#}else{#
				#tails. do nothing.#
			#}#
		}#
	}#
	return(thedata)#
}
sample_cluster = function(thedata,threshold){#
	#This function does not assume any particular structure to the data, but it calls resample_data and get_stats which do#
	##
	#thedata = the data that you want to resample#
	#print(thedata[1:3,])#
	print(1)#
	sampled_data<-resample_data(thedata)#
	#print(sampled_data[1:3,])#
	sigs_me<-get_stats(sampled_data)#
	sampled_clusters<-cluster(sigs_me$z,threshold)#
	sampled_cluster_scores<-score_clusters(sigs_me$z,sampled_clusters)#
	return(max(sampled_cluster_scores$score))#
}#
#
sample_clusters = function(thedata,n,multi,threshold){#
	#This function does not assume any particular structure to the data, but it calls resample_data and get_stats (via sample_cluster), which do#
	##
	#thedata = the data that you want to resample#
	#n = number of times to resample#
	#if multi==TRUE, use multiple processors (will need to have loaded the appropriate libraries)#
#
	require(multicore)#
	require(doMC)#
	require(foreach)#
#
	if (multi){#
		registerDoMC(cores=3) #how many processes to spawn?	was written for a computer with 4 processors. This keeps 1 free for other uses.#
		clusters<-foreach(i=iter(c(1:n)),.combine=rbind) %dopar% sample_cluster(thedata,threshold)#
	}else{#
		clusters<-foreach(i=iter(c(1:n)),.combine=rbind) %do% sample_cluster(thedata,threshold)#
	}#
	return(clusters)#
}
# This R script uses JoshH's scripts to create a set of fake datasets, and then analyzes#
# those datasets using three different threshold.#
# j is number of simulations#
#
threshold = c(1.6,2.0,2.4)#
real_score = matrix(NA,1,3)#
pval <- matrix(NA,1000,3)#
#
#cluster the data#
for (j in c(1:1000)){#
print(j)#
mydata <- createdata()#
sigs_me<-get_stats(mydata)#
#
for (k in c(1:length(threshold))){ # k is number of thresholds#
actual_clusters<-cluster(sigs_me$z,threshold[k]) #cluster the actual data#
actual_cluster_scores<-score_clusters(sigs_me$z,actual_clusters) #find the scores for the clusters in the actual data#
real_score[1,k] <- max(actual_cluster_scores$score)#
sampled_maxes<-sample_clusters(mydata,1000,TRUE,threshold[k])#
pval[j,k] = 1 - sum(as.numeric(max(actual_cluster_scores$score) > c(max(actual_cluster_scores$score),sampled_maxes[,1])))/(length(sampled_maxes[,1])+1)#
#
}#
}#
#for (k in c(1:length(threshold))){ # k is number of thresholds#
#	sampled_maxes<-sample_clusters(mydata,1000,TRUE,threshold[k])#
#	pval[j,k] = 1 - sum(as.numeric(max(actual_cluster_scores$score) > c(max(actual_cluster_scores$score),sampled_maxes[,1])))/(length(sampled_maxes[,1])+1)#
#	#
#	}#
#}
logit(0)
library(arm)
logit(0)
invlogit(0)
?setwd
#values used for longdiff#
# s_n<-24 #how many subjects#
# switch_baseline<-.25 #probability of switching to target at baseline#
# switch_on<-.5 #probability of switching to target when effect is in effect#
# switch_off<-.1 #probability of switching off target when effect is in effect#
# items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
# bins_n<-20 #how many bins?#
# first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
# time_diff<-2.5 #how many bins earlier is there an effect in condition 1 than condition 2?#
# noise<-2 #trial noise sd#
# sub_speeds<-rnorm(s_n,0,1) #how much faster or slower than typical is the subject?#
# sub_s_sd<-1 #sd for sub speed error#
# sub_effects<-rnorm(s_n,0,1.5) #degree to which effect size is larger or smaller for that subject#
# sub_e_sd<-1 #sd for sub effect error#
# item_effects<-rnorm(items_n,0,.5) #degree to which effect size is larger or smaller than typical for that item#
# item_sd<-1 #sd for item effect error#
#
#values used for shortdiff#
s_n<-24 #how many subjects#
switch_baseline<-.25 #probability of switching to target at baseline#
switch_on<-.9 #probability of switching to target when effect is in effect#
switch_off<-.2 #probability of switching off target when effect is in effect#
items_n<-5 #how many items per condition? 2-condition experiment. items appear in both conditions, within subject#
bins_n<-20 #how many bins?#
first_bin<-7 #on average, what is the first bin to show an effect in condition with early effect?#
time_diff<-0 #how many bins earlier is there an effect in condition 1 than condition 2?#
noise<-.25 #trial noise sd#
sub_speeds<-rnorm(s_n,0,.25) #how much faster or slower than typical is the subject?#
sub_s_sd<-.5 #sd for sub speed error#
sub_effects<-rnorm(s_n,0,2) #degree to which effect size is larger or smaller for that subject#
sub_e_sd<-.25 #sd for sub effect error#
item_effects<-rnorm(items_n,0,2) #degree to which effect size is larger or smaller than typical for that item#
item_sd<-.25 #sd for item effect error#
#what's alpha?#
1-.95^(1/bins_n)#
#
createdata <- function(){#
mydata<-data.frame(subject=c(NA),condition=c(NA),item=c(NA),bin=c(NA),fixtarg=c(NA))#
for (sub in c(1:s_n)){#
	for (item in c(1:items_n)){#
		#do a item for each condition simultaneously (saves time)#
		#draw a bin for an effect#
		con1_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise)#
		con2_bin<-first_bin+rnorm(1,sub_speeds[sub],sub_s_sd)+rnorm(1,0,noise) #+time_diff+rnorm(1,sub_effects[sub],sub_e_sd)+rnorm(1,item_effects[item],item_sd)#
			current_1<-rbinom(1,1,.5) #where am I looking in first bin for condition 1?#
			current_2<-rbinom(1,1,.5) #where am I looking in first bin for condition 2?#
			mydata<-rbind(mydata,c(sub,1,item,1,current_1))#
			mydata<-rbind(mydata,c(sub,2,item,1,current_2))#
			for (bin in c(2:bins_n)){#
				if (bin<con1_bin){#
						if (rbinom(1,1,switch_baseline)){current_1<-(1-current_1)}#
					}else{#
						if (current_1){#
							if (rbinom(1,1,switch_off)){current_1<-(1-current_1)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_1<-(1-current_1)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,1,item,bin,current_1))#
				if (bin<con2_bin){#
						if (rbinom(1,1,switch_baseline)){current_2<-(1-current_2)}#
					}else{#
						if (current_2){#
							if (rbinom(1,1,switch_off)){current_2<-(1-current_2)}						#
						}else{#
							if (rbinom(1,1,switch_on)){current_2<-(1-current_2)}													#
						}#
				}#
				mydata<-rbind(mydata,c(sub,2,item,bin,current_2))#
			}#
	}#
}#
return(mydata<-mydata[is.na(mydata$subject)==FALSE,])#
}#
mydata <- createdata()#
#
#calculate means#
submeans<-aggregate(mydata$fixtarg,by=list(mydata$subject,mydata$condition,mydata$bin),FUN=mean)#
colnames(submeans)<-c("subject","condition","bin","fixtarg")#
means<-aggregate(submeans$fixtarg,by=list(submeans$condition,submeans$bin),FUN=mean)#
colnames(means)<-c("condition","bin","fixtarg")#
temp<-aggregate(submeans$fixtarg,by=list(submeans$condition,submeans$bin),FUN=sd)#
means$se<-temp[,3]/(s_n^.5)
summary(mydata)
0.05/20
logit(1)
log((1/0))
log((0.95/0.05))
?names
load("/Users/hrabagli/Documents/Studies/Sense Resolution/3_Online_ET/ETAutismData/FullStats_April1.RDATA")
ls()
Full.Clusters
3*12
36+7
43/2
36+8
/2
44/22
?grepl
a = ("aa","bb","ss","ba")
a = c("aa","bb","ss","ba")
a
grep(a,"b")
grep("b",a)
grep("b",a) -1
a[(grep("b",a) -1)]
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al. #
#
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)#
#
#Bayes factor -- minimum effect of 0.01, maximum of 0.06, our effect = -0.03519684 and our SE = -0.03/-1.7874=  0.01678416#
#
# lmer (Rabag style)#
sense.pop.sklar.raw <- summary(lmer(rt ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))#
print(sense.pop.sklar.raw)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.raw)[,3]))))#
#
sense.pop.sklar.log <- summary(lmer(log(rt) ~ Condition + (1+Condition|SubjNo)+ (1|prime), data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control"))))#
print(sense.pop.sklar.log)#
print(paste("p value = ", 2*pnorm(-abs(coef(sense.pop.sklar.log)[,3]))))
library(permute)
?sample
subj.list <- unique(sense.pop.sklar$SubjNo)
t.test(rt ~ Condition, data = subset(sense.pop.sklar.summary, SubjNo %in% sample(subj.list,31)), paired = T)
t.test(rt ~ Condition, data = subset(sense.pop.sklar.summary, SubjNo %in% sample(subj.list,31)), paired = T) -> a
summary(a)
a$p.value
t.test(rt ~ Condition, data = subset(sense.pop.sklar.summary, SubjNo %in% sample(subj.list,31)), paired = T)$p.value
a = rep(NA, 1000)#
subj.list <- unique(sense.pop.sklar$SubjNo)#
for (i in 1:1000){#
a[i] <- t.test(rt ~ Condition, data = subset(sense.pop.sklar.summary, SubjNo %in% sample(subj.list,31)), paired = T)$p.value#
}
hist(a)
a = rep(NA, 1000)#
subj.list <- unique(sense.pop.sklar$SubjNo)#
for (i in 1:1000){#
a[i] <- t.test(rt ~ Condition, data = subset(sense.pop.sklar.summary, SubjNo %in% sample(subj.list,20)), paired = T)$p.value#
}
?hist
hist(a, add = T)
hist(a, add = T, col = "grey")
b = rep(NA, 1000)#
subj.list <- unique(sense.pop.sklar$SubjNo)#
for (i in 1:1000){#
b[i] <- t.test(rt ~ Condition, data = subset(sense.pop.sklar.summary, SubjNo %in% sample(subj.list,31)), paired = T)$p.value#
}
hist(a, col=rgb(0.1,0.1,0.1,0.5),xlim=c(0,10), ylim=c(0,200), main=Overlapping Histogram)#
hist(b, col=rgb(0.8,0.8,0.8,0.5), add=T)#
box()
hist(a, col=rgb(0.1,0.1,0.1,0.5), main=Overlapping Histogram)#
hist(b, col=rgb(0.8,0.8,0.8,0.5), add=T)#
box()
summary(a)
hist(a, col=rgb(0.1,0.1,0.1,0.5))#
hist(b, col=rgb(0.8,0.8,0.8,0.5), add=T)#
box()
hist(a, col=rgb(0.1,0.1,0.1,0.5),xlim=c(0,1), ylim=c(0,200))#
hist(b, col=rgb(0.8,0.8,0.8,0.5), add=T)#
box()
hist(a, col=rgb(0.1,0.1,0.1,0.5),xlim=c(0,1), ylim=c(0,400))#
hist(b, col=rgb(0.8,0.8,0.8,0.5), add=T)#
box()
hist(a, col=rgb(0.1,0.1,0.1,0.5),xlim=c(0,1), ylim=c(0,400))
hist(b, col=rgb(0.8,0.8,0.8,0.5), add=T)
summary(sense)
summary(sense.pop.sklar)
?read.csv
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T, nrows = 120)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al.
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T, nrows = 240)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al.
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T, nrows = 100)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al.
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
summary(sense.pop)
summary(lmer(rt~ Condition + (1+Condition|SubjNo), data = sense.pop))
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al.
summaryBy(perceptual.rating ~ Condition, data = sense.pop)
summary(lmer(perceptual.rating ~ Condition + (1+Condition|SubjNo), data = subset(sense.pop, Condition %in% c("Sklar_filler","Sklar_violation"))))
qqplot(sense.pop$rt, sense.pop$rt)
?qqplot
qqnorm(sense.pop$rt)
hist(sense.pop$rt)
qqnorm(sense.pop$rt)
qqnorm(log(sense.pop$rt))
qqnorm(sqrt(sense.pop$rt))
qqnorm(exp(sense.pop$rt))
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
library(plyr)#
library(lme4)#
library(doBy)#
library(ggplot2)#
#
read_data <- function(path_name){#
list.files(path = path_name,full.names = T, pattern = ".csv") -> file_list#
comp = c()#
for (x in file_list){#
	data <- read.csv(x,header = T)#
	if ("perceptual.rating.reactiontime" %in% colnames(data)){ #
		data <- subset(data, select = -perceptual.rating.reactiontime)#
		}#
		if ("X" %in% colnames(data)){ #
		data <- subset(data, select = -X)#
		data$rt <- as.character(data$rt)#
		}#
	comp <- rbind(comp, data)#
	}#
	return(comp)#
}#
#
sense.pop <- read_data("./data/")#
#
# Make RTs numeric [need to remove timeout "none" responses to 8s]#
sense.pop <- subset(sense.pop, rt != "None")#
sense.pop$rt <- as.numeric(sense.pop$rt)#
sense.pop$Length <- nchar(as.character(sense.pop$prime),allowNA = T)#
sense.pop$Condition <- as.character(sense.pop$prime_semantics)#
sense.pop[sense.pop$prime_semantics %in% c("Sklar_control_A","Sklar_control_B"),]$Condition <- "Sklar_control"#
sense.pop$Condition <- as.factor(sense.pop$Condition)#
# Note that this analysis includes all of the inclusion criteria discussed by Sklar et al. #
#
###########################################################################################################################
##
# Let's first analyze for the Sklar trials#
sense.pop.sklar <- subset(sense.pop, Condition %in% c("Sklar_control", "Sklar_violation")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < 3sd above group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.sklar), keep.names = T)#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
#
sense.pop.sklar <- subset(sense.pop.sklar, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.sklar <- subset(sense.pop.sklar, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.sklar <- ddply(sense.pop.sklar, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
#
# Remove RTs < 200ms#
sense.pop.sklar <- subset(sense.pop.sklar, rt > 0.2)
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
# T test (Sklar style)#
sense.pop.sklar.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.sklar,  Condition %in% c("Sklar_violation", "Sklar_control")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.sklar.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
t.test(exp(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
t.test(sqrt(rt) ~ Condition, data = sense.pop.sklar.summary, paired = T)
sense.pop.new <- subset(sense.pop, Condition %in% c("Sensible", "Non-sensible")) #
#
# Remove outlier subjects by Accuracy and RT (mean acc must be > 0.9, mean rt must be < (!!, see by trial exclusion below) 3sd from group mean)#
Acc <- summaryBy(match. + rt ~ SubjNo, data = subset(sense.pop.new), keep.names = T)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$match. > 0.9,]$SubjNo)#
sense.pop.new <- subset(sense.pop.new, SubjNo %in% Acc[Acc$rt < (mean(Acc$rt) + (3*sd(Acc$rt))),]$SubjNo)#
#
# Remove incorrect trials#
sense.pop.new <- subset(sense.pop.new, match. == 1)#
#
# Remove outliers by subject (less than 3sd from participant mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(SubjNo), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# [for Expt 1 only] Remove outliers by condition (less than 3sd from condition mean -- note that this is not a symmetric exclusion criterion)#
sense.pop.new <- ddply(sense.pop.new, .(Condition), function(d){ #
	by_subj_include <- mean(d$rt, na.rm = T) + 3*c(-1,1)*sd(d$rt,na.rm = T)#
	d = subset(d, rt < by_subj_include[2])#
	})#
# Remove RTs < 200ms#
sense.pop.new <- subset(sense.pop.new, rt > 0.2)#
#
# T test (Sklar style)#
sense.pop.new.summary <- summaryBy(rt ~ SubjNo + Condition, data = subset(sense.pop.new,  Condition %in% c("Non-sensible","Sensible")), keep.names = T)#
t.test(rt ~ Condition, data = sense.pop.new.summary, paired = T)#
t.test(log(rt) ~ Condition, data = sense.pop.new.summary, paired = T)
t.test(exp(rt) ~ Condition, data = sense.pop.new.summary, paired = T)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
library(ez)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
    d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
    output <- cbind(key = d$key_press,Subj = d$Subj)#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
Comp_Import = function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
    d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj"),]#
    #d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
    d$Cond <- as.factor(d$stims$Cond)#
    d$Item <- as.factor(d$stims$Item)#
    d$Task <- "List"#
    d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
    d$Task <- as.factor(d$Task)#
    d$Stim <- "One Word"#
    d[d$Cond %in% c("Match-Adj", "Match-List","Mismatch-List","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
    d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
    d$Match <- "Match"#
    d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun"),]$Match <- "MisMatch"#
    d$Match <- as.factor(d$Match)#
    output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
    output$rt <- as.numeric(as.character(output$rt))#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
#
# Function for plotting data using bar plots#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  DV.se <- DV.se/(sqrt( (length(unique(Subj))) ))#
  comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  if (leg == TRUE){#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
  } else{#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
    axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
  }#
  arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./data")#
print(catch)#
comp <- Comp_Import("./data")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Calcs for within subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
  comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
  comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
}#
#
# RT Analyses#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Acc Analyses#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA#
#
# Prepare variables for bar graph#
comp$DetailedTask <- "List (Cup,Tree)"#
comp[comp$Task == "Phrase",]$DetailedTask <- "Phrase (Pink Tree)"#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)", "List (Cup,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("Two Words", "One Word"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim   , data = comp, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(700,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
#
# Prepare variables for bar graph#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)","List (Cup,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("One Word", "Two Words"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
# Print RT means and c.i.s#
ci.m <- aggregate(rt ~  Stim + DetailedTask , comp.rt, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.high); ci.h#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim   , data = comp, FUN = c(mean,sd), na.rm = T )#
#
# Function for plotting data as a ling graph#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  theme_set(theme_bw())#
  DV.se <- DV.se/(sqrt(length(unique(Subj))))#
  #comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  #comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
  print((graph_data))#
  if (leg == TRUE){#
    #x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.8), tck = -0.01)#
    # PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
    #		#
    x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
      ylim(ylimit) +#
      ylab(ylab) +#
      geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
      geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
      guides(colour=guide_legend()) +#
      theme(strip.background = element_rect(fill="#FFFFFF"), #
            strip.text = element_text(size=12), #
            axis.text = element_text(size=12),#
            axis.title = element_text(size=14),#
            legend.text = element_text(size=12),#
            legend.key = element_blank(),#
            legend.title=element_blank(),#
            title = element_text(size=16),#
            panel.grid = element_blank(),#
            axis.title.x=element_blank(),#
            legend.position=c(0.3,0.8))#
  } else{#
    x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
      ylim(ylimit) +#
      ylab(ylab) +#
      geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
      geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
      theme(strip.background = element_rect(fill="#FFFFFF"), #
            strip.text = element_text(size=12), #
            axis.text = element_text(size=12),#
            axis.title = element_text(size=14),#
            title = element_text(size=16),#
            panel.grid = element_blank(),#
            axis.text.x=element_blank(),#
            axis.title.x=element_blank(),#
            legend.position= "none")}#
  #arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
  return(x)#
}#
RT <- Comp_Graph_l(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(700,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.7,1.03),"Accuracy")#
#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc,gRT, nrow = 2, heights = c(1,2))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
library(ez)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
    d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
    output <- cbind(key = d$key_press,Subj = d$Subj)#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
Comp_Import = function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
    d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj"),]#
    #d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
    d$Cond <- as.factor(d$stims$Cond)#
    d$Item <- as.factor(d$stims$Item)#
    d$Task <- "List"#
    d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
    d$Task <- as.factor(d$Task)#
    d$Stim <- "One Word"#
    d[d$Cond %in% c("Match-Adj", "Match-List","Mismatch-List","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
    d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
    d$Match <- "Match"#
    d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun"),]$Match <- "MisMatch"#
    d$Match <- as.factor(d$Match)#
    output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
    output$rt <- as.numeric(as.character(output$rt))#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
#
# Function for plotting data using bar plots#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  DV.se <- DV.se/(sqrt( (length(unique(Subj))) ))#
  comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  if (leg == TRUE){#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
  } else{#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
    axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
  }#
  arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1a")#
print(catch)#
comp <- Comp_Import("./Exp1a")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Calcs for within subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
  comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
  comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
}#
#
# RT Analyses#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Acc Analyses#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA#
#
# Prepare variables for bar graph#
comp$DetailedTask <- "List (Cup,Tree)"#
comp[comp$Task == "Phrase",]$DetailedTask <- "Phrase (Pink Tree)"#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)", "List (Cup,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("Two Words", "One Word"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim   , data = comp, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(700,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
#
# Prepare variables for bar graph#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)","List (Cup,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("One Word", "Two Words"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
# Print RT means and c.i.s#
ci.m <- aggregate(rt ~  Stim + DetailedTask , comp.rt, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.high); ci.h#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim   , data = comp, FUN = c(mean,sd), na.rm = T )#
#
# Function for plotting data as a ling graph#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  theme_set(theme_bw())#
  DV.se <- DV.se/(sqrt(length(unique(Subj))))#
  #comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  #comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
  print((graph_data))#
  if (leg == TRUE){#
    #x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.8), tck = -0.01)#
    # PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
    #		#
    x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
      ylim(ylimit) +#
      ylab(ylab) +#
      geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
      geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
      guides(colour=guide_legend()) +#
      theme(strip.background = element_rect(fill="#FFFFFF"), #
            strip.text = element_text(size=12), #
            axis.text = element_text(size=12),#
            axis.title = element_text(size=14),#
            legend.text = element_text(size=12),#
            legend.key = element_blank(),#
            legend.title=element_blank(),#
            title = element_text(size=16),#
            panel.grid = element_blank(),#
            axis.title.x=element_blank(),#
            legend.position=c(0.3,0.8))#
  } else{#
    x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
      ylim(ylimit) +#
      ylab(ylab) +#
      geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
      geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
      theme(strip.background = element_rect(fill="#FFFFFF"), #
            strip.text = element_text(size=12), #
            axis.text = element_text(size=12),#
            axis.title = element_text(size=14),#
            title = element_text(size=16),#
            panel.grid = element_blank(),#
            axis.text.x=element_blank(),#
            axis.title.x=element_blank(),#
            legend.position= "none")}#
  #arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
  return(x)#
}#
RT <- Comp_Graph_l(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(700,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.7,1.03),"Accuracy")#
#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc,gRT, nrow = 2, heights = c(1,2))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask*Stim|SubjNo), data = comp))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask*Stim|SubjNo), data = comp, family = "binomial"))
summary(comp)
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask*Stim|Subj), data = comp, family = "binomial"))
summary(lmer(rt ~ DetailedTask * Stim + (1+DetailedTask*Stim|Subj), data = comp))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedTask * Stim + (1*Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask*Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask+Stim|Subj), data = comp, family = "binomial"))
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data[4]$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Disjunction"#
	#print(summary(d))#
	d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "One Word"#
	d[d$Cond %in% c("Match-Adj", "Match-Noun","Match-Color","Mismatch-Disjunc","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
	d$Match <- "Match"#
	d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun","Mismatch-Disjunc"),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.2), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1b")#
print(catch)#
comp <- Comp_Import("./Exp1b")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Prep for w-subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "Disjunction"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "Disjunction"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA#
#Prep data for bar graph#
comp$DetailedTask <- "List (Pink,Tree)"#
comp[comp$Task == "Phrase",]$DetailedTask <- "Phrase (Pink Tree)"#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)", "List (Pink,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("Two Words", "One Word"))#
comp.rt <- summaryBy(rt + rtAdj ~ Task +DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
comp.rt <- summaryBy(rt + rtAdj ~ Task +DetailedTask+ Stim  , , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
#
comp.Acc <- summaryBy(Acc + AccAdj~  Task +DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  Task +DetailedTask + Stim  , , data = comp, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(650,900),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
# Prep data for line graph#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)", "List (Pink,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("One Word", "Two Words"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ Task +DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
# Print means and cis#
ci.m <- aggregate(rt ~  Stim + DetailedTask , comp.rt, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.high); ci.h#
comp.rt <- summaryBy(rt + rtAdj ~ Task +DetailedTask+ Stim  , , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
comp.Acc <- summaryBy(Acc + AccAdj~  Task +DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  Task +DetailedTask + Stim  , , data = comp, FUN = c(mean,sd), na.rm = T )#
#
# Function for plotting data in line graph#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
theme_set(theme_bw())#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
#
graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
print((graph_data))#
if (leg == TRUE){#
# PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
#
		guides(colour=guide_legend()) +#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        legend.text = element_text(size=12),#
        legend.key = element_blank(),#
       legend.title=element_blank(),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
         axis.title.x=element_blank(),#
        legend.position=c(0.3,0.8))#
} else{#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
        axis.text.x=element_blank(),#
       axis.title.x=element_blank(),#
  	   	legend.position= "none")}#
return(x)#
}#
RT <- Comp_Graph_l(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(650,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.7,1.03),"Accuracy")#
#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc,gRT, nrow = 2, heights = c(1,2))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask+Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask*Stim|Subj), data = comp, family = "binomial"))
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
ezANOVA(subset(comp, Acc ==1 & Match == "Match"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Color"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "FixedColor"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
# Prep for bar graph#
comp$DetailedTask <- "Matched Predictability (Pink Tree)"#
comp[comp$Type == "Color",]$DetailedTask <- "Mismatched Predictability (Pink Tree)"#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Mismatched Predictability (Pink Tree)","Matched Predictability (Pink Tree)"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ PicType + DetailedTask + Stim + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
comp.rt <- summaryBy(rt + rtAdj ~ PicType + DetailedTask + Stim , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
comp.Acc <- summaryBy(Acc + AccAdj~  PicType + DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  PicType + DetailedTask + Stim   , data = comp.Acc, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(650,900),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
#
# Prep for line graph#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Matched Predictability (Pink Tree)","Mismatched Predictability (Pink Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("One Word", "Two Words"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ PicType + DetailedTask + Stim + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
ci.m <- aggregate(rt ~  Stim + DetailedTask , comp.rt, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.high); ci.h#
comp.rt <- summaryBy(rt + rtAdj ~ PicType + DetailedTask + Stim , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
comp.Acc <- summaryBy(Acc + AccAdj~  PicType + DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  PicType + DetailedTask + Stim   , data = comp.Acc, FUN = c(mean,sd), na.rm = T )#
#
# Function for plotting data#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
theme_set(theme_bw())#
S <- length(unique(Subj))/2#
DV.se <- DV.se/(sqrt(S))#
#comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
#comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
print((S))#
if (leg == TRUE){#
#x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.8), tck = -0.01)#
# PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
#
#		#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
#
		guides(colour=guide_legend()) +#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        legend.text = element_text(size=12),#
        legend.key = element_blank(),#
       legend.title=element_blank(),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
         axis.title.x=element_blank(),#
        legend.position=c(0.43,0.8))#
} else{#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
        axis.text.x=element_blank(),#
       axis.title.x=element_blank(),#
  	   	legend.position= "none")}#
#arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
return(x)#
}#
RT <- Comp_Graph_l(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(650,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.7,1.05),"Accuracy")#
#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc,gRT, nrow = 2, heights = c(1,2))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask*Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask+Stim|Subj), data = comp, family = "binomial"))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
#Reaction Times#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big" & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark" & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Accuracy#
ezANOVA(comp.omni, Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
comp.omni$rtAdj <- NA#
comp.omni$AccAdj <- NA#
for (i in unique(comp.omni$Subj)){#
	comp.omni[comp.omni$Subj == i,]$rtAdj <- ((comp.omni[comp.omni$Subj == i,]$rt - mean(comp.omni[comp.omni$Subj == i,]$rt, na.rm = T)) + mean(comp.omni$rt, na.rm = T))#
	comp.omni[comp.omni$Subj == i,]$AccAdj <- ((comp.omni[comp.omni$Subj == i,]$Acc - mean(comp.omni[comp.omni$Subj == i,]$Acc, na.rm = T)) + mean(comp.omni$Acc, na.rm = T))#
	}#
#
# Prep for bar graph#
comp.omni$DetailedType <- "Complex (Dark Pink Tree)"#
comp.omni[comp.omni$Type == "Big",]$DetailedType <- "Simple (Big Pink Tree)"#
comp.omni$DetailedType <- ordered(comp.omni$DetailedType, levels = c("Simple (Big Pink Tree)", "Complex (Dark Pink Tree)"))#
comp.omni$Stim <- ordered(comp.omni$Stim, levels = c( "One Word","Two Words","Three Words"))#
#
comp.omni.rt <- summaryBy(rt + rtAdj ~ Type+ DetailedType + Stim + Subj, , data = subset(comp.omni, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
comp.omni.rt <- summaryBy(rt + rtAdj ~ Type+ DetailedType + Stim , , data = comp.omni.rt, FUN = c(mean,sd), na.rm = T )#
print(comp.omni.rt)#
#
comp.omni.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType+ Stim  +Subj, , data = comp.omni, FUN = c(mean), na.rm = T , keep.names = T)#
comp.omni.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType+ Stim  , , data = comp.omni.Acc, FUN = c(mean,sd), na.rm = T )#
print(comp.omni.Acc)#
#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.omni.rt$rt.mean,comp.omni.rt$rtAdj.sd, comp.omni.rt$Stim, comp.omni.rt$Type, comp.omni$Subj, paste("Three Words", "Reaction Time", sep = " "), c(650,900),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.omni.Acc$Acc.mean,comp.omni.Acc$AccAdj.sd, comp.omni.Acc$Stim, comp.omni.Acc$Type, comp.omni$Subj, paste("Three Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
# Prep for line graph#
comp.omni$Stim <- ordered(comp.omni$Stim,  levels = c("One Word", "Two Words", "Three Words"))#
comp.omni$DetailedType <- ordered(comp.omni$DetailedType, levels = c("Complex (Dark Pink Tree)","Simple (Big Pink Tree)"))#
comp.omni.rt <- summaryBy(rt + rtAdj ~ Type+ DetailedType + Stim + Subj, , data = subset(comp.omni, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
ci.m <- aggregate(rt ~  Stim + DetailedType , comp.omni.rt, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedType , comp.omni.rt, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedType , comp.omni.rt, ci.high); ci.h#
#
comp.omni.rt <- summaryBy(rt + rtAdj ~ Type+ DetailedType + Stim , , data = comp.omni.rt, FUN = c(mean,sd), na.rm = T )#
#
comp.omni.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType+ Stim  +Subj, , data = comp.omni, FUN = c(mean), na.rm = T , keep.names = T)#
comp.omni.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType+ Stim  , , data = comp.omni.Acc, FUN = c(mean,sd), na.rm = T )#
#
# Function for plotting data#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
theme_set(theme_bw())#
DV.se <- DV.se/(sqrt((length(unique(Subj))/2)))#
graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
print((graph_data))#
if (leg == TRUE){#
# PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
#
#		#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
#
		guides(colour=guide_legend()) +#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        legend.text = element_text(size=12),#
        legend.key = element_blank(),#
       legend.title=element_blank(),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
         axis.title.x=element_blank(),#
        legend.position=c(0.2,0.8))#
} else{#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
        axis.text.x=element_blank(),#
       axis.title.x=element_blank(),#
  	   	legend.position= "none")}#
return(x)#
}#
RT <- Comp_Graph_l(comp.omni.rt$rt.mean,comp.omni.rt$rtAdj.sd, comp.omni.rt$Stim, comp.omni.rt$DetailedType, comp.omni$Subj, paste("Three Words", "Reaction Time", sep = " "), c(650,800),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.omni.Acc$Acc.mean,comp.omni.Acc$AccAdj.sd, comp.omni.Acc$Stim, comp.omni.Acc$DetailedType, comp.omni$Subj, paste("Three Words", "Accuracy", sep = " "), c(0.7,1),"Accuracy")#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc, gRT, nrow = 2, heights = c(1,2))
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask+Stim|Subj), data = comp, family = "binomial"))
summary(comp)
summary(glmer(Acc ~ DetailedTask * Stim + (1+DetailedTask+Stim|Subj), data = comp.omni, family = "binomial"))
summary(comp.omni)
summary(glmer(Acc ~ DetailedType * Stim + (1+DetailedType+Stim|Subj), data = comp.omni, family = "binomial"))
summary(glmer(Acc ~ DetailedType * Stim + (1+DetailedType*Stim|Subj), data = comp.omni, family = "binomial"))
summary(glmer(Acc ~ DetailedType * Stim + (1+Stim|Subj), data = comp.omni, family = "binomial"))
contrasts(comp.omni$DetailedType)
contrasts(comp.omni$Stim)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d[d$Length == "3",]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Striped"#
	d[grep("spotted",d$stims$Pic, ignore.case = TRUE),]$PicType <- "Spotted"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block = rep(c(1,2), each = 150)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Block = d$Block, File = d$stimulus)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.5), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp3")#
print(catch)#
comp <- Comp_Import("./Exp3")#
comp <- subset(comp, Type %in% c("Adj3","Adj4", "Adv4"))#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
# Reaction Times#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4") & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj4"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adv4"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" & Stim != "Three Words" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj4" & Stim != "Three Words" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adv4" & Stim != "Three Words" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" & Stim != "One Word" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj4" & Stim != "One Word" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adv4" & Stim != "One Word" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
# Accuracy#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4")), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4") & Stim != "One Word"), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Two Words"), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Three Words"), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
# Prep for bar graph#
comp$DetailedType <- "Complex Adjective (Big Spotted Tree)"#
comp[comp$Type == "Adj4",]$DetailedType <- "Simple (Big Spotted Tree)"#
comp[comp$Type == "Adv4",]$DetailedType <- "Complex Compound Adj. (Big Spotted Tree)"#
comp$DetailedType <- ordered(comp$DetailedType, levels = c("Simple (Big Spotted Tree)","Complex Adjective (Big Spotted Tree)","Complex Compound Adj. (Big Spotted Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("Three Words", "Two Words","One Word"))#
#
comp.rt1 <- summaryBy(rt + rtAdj ~ Type + DetailedType + Stim + Subj, , data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4","Adv4")), FUN = c(mean), na.rm = T , keep.names = T)#
comp.rt1$Type <- ordered(comp.rt1$Type, levels = c("Adj4", "Adj3","Adv4"))#
comp.rt <- summaryBy(rt + rtAdj ~ Type + DetailedType  + Stim , data = comp.rt1, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
#
comp.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType  + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc$Type <- ordered(comp.Acc$Type, levels = c("Adj4", "Adj3","Adv4"))#
comp.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType  + Stim   , data = comp.Acc, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$Type, comp$Subj, paste("Three Words", "Reaction Time", sep = " "), c(750,1050),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$Type, comp$Subj, paste("Three Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
#
#Quick analysis on whether the effect differs depending on whether the size of the image and the size of the texture are congruent#
comp$Cong <- 0#
comp[grep("big_big",comp$Pic, ignore.case = TRUE),]$Cong <- 1#
comp[grep("small_small",comp$Pic, ignore.case = TRUE),]$Cong <- 1#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" & Cong ==1 ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" & Cong ==0 ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Prep for line graph#
comp$DetailedType <- ordered(comp$DetailedType, levels = c("Complex Adjective (Big Spotted Tree)","Complex Compound Adj. (Big Spotted Tree)","Simple (Big Spotted Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("One Word", "Two Words", "Three Words"))#
comp.rt1 <- summaryBy(rt + rtAdj ~ Type + DetailedType + Stim + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
ci.m <- aggregate(rt ~  Stim + DetailedType , comp.rt1, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedType , comp.rt1, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedType , comp.rt1, ci.high); ci.h#
comp.rt1$Type <- ordered(comp.rt1$Type, levels = c("Adj4", "Adj3"))#
comp.rt <- summaryBy(rt + rtAdj ~ Type + DetailedType  + Stim , data = comp.rt1, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
#
comp.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType  + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc$Type <- ordered(comp.Acc$Type, levels = c("Adj4", "Adj3"))#
comp.Acc <- summaryBy(Acc + AccAdj~  Type + DetailedType  + Stim   , data = comp.Acc, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
# Function for plotting data#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
theme_set(theme_bw())#
DV.se <- DV.se/(sqrt((length(unique(Subj))/3)))#
#comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
#comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
print((graph_data))#
if (leg == TRUE){#
#x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.8), tck = -0.01)#
# PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
#
#		#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
#
		guides(colour=guide_legend()) +#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        legend.text = element_text(size=12),#
        legend.key = element_blank(),#
       legend.title=element_blank(),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
         axis.title.x=element_blank(),#
        legend.position=c(0.35,0.8))#
} else{#
x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
		ylim(ylimit) +#
		ylab(ylab) +#
		geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
		geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
		theme(strip.background = element_rect(fill="#FFFFFF"), #
        strip.text = element_text(size=12), #
        axis.text = element_text(size=12),#
        axis.title = element_text(size=14),#
        title = element_text(size=16),#
        panel.grid = element_blank(),#
        axis.text.x=element_blank(),#
       axis.title.x=element_blank(),#
  	   	legend.position= "none")}#
#arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
return(x)#
}#
RT <- Comp_Graph_l(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedType, comp$Subj, paste("Three Words", "Reaction Time", sep = " "), c(700,1050),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedType, comp$Subj, paste("Three Words", "Accuracy", sep = " "), c(0.7,1),"Accuracy")#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc, gRT, nrow = 2, heights = c(1,2))
summary(glmer(Acc ~ DetailedType * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedType * Stim + (1|Subj), data = comp, family = "binomial"))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
#Reaction Times#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big" & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark" & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Accuracy#
ezANOVA(comp.omni, Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA
summary(glmer(Acc ~ DetailedType * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ DetailedType * Stim + (1+Stim|Subj), data = comp.omni, family = "binomial"))
summary(comp.omni)
summary(glmer(Acc ~ Type * Stim + (1+Stim|Subj), data = comp.omni, family = "binomial"))
summary(glmer(Acc ~ Type * Stim + (1|Subj), data = comp.omni, family = "binomial"))
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
ezANOVA(subset(comp, Acc ==1 & Match == "Match"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Color"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "FixedColor"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA
summary(glmer(Acc ~ Task * Stim + (1+Task|Subj), data = comp, family = "binomial"))
summary(comp)
summary(glmer(Acc ~ Type * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data[4]$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Disjunction"#
	#print(summary(d))#
	d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "One Word"#
	d[d$Cond %in% c("Match-Adj", "Match-Noun","Match-Color","Mismatch-Disjunc","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
	d$Match <- "Match"#
	d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun","Mismatch-Disjunc"),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.2), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1b")#
print(catch)#
comp <- Comp_Import("./Exp1b")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Prep for w-subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "Disjunction"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "Disjunction"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA
summary(glmer(Acc ~ Type * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Type * Stim + (1+Stim*Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim*Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim+Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
library(ez)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
    d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
    output <- cbind(key = d$key_press,Subj = d$Subj)#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
Comp_Import = function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
    d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj"),]#
    #d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
    d$Cond <- as.factor(d$stims$Cond)#
    d$Item <- as.factor(d$stims$Item)#
    d$Task <- "List"#
    d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
    d$Task <- as.factor(d$Task)#
    d$Stim <- "One Word"#
    d[d$Cond %in% c("Match-Adj", "Match-List","Mismatch-List","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
    d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
    d$Match <- "Match"#
    d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun"),]$Match <- "MisMatch"#
    d$Match <- as.factor(d$Match)#
    output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
    output$rt <- as.numeric(as.character(output$rt))#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
#
# Function for plotting data using bar plots#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  DV.se <- DV.se/(sqrt( (length(unique(Subj))) ))#
  comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  if (leg == TRUE){#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
  } else{#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
    axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
  }#
  arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1a")#
print(catch)#
comp <- Comp_Import("./Exp1a")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Calcs for within subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
  comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
  comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
}#
#
# RT Analyses#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Acc Analyses#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA
summary(glmer(Acc ~ Task * Stim + (1+Stim*Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim+Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
contrasts(comp$Stim)
contrasts(comp$Task)
contrasts(comp$Task)[1] <- -0.5
contrasts(comp$Task)[2] <- 0.5
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim+Task|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim+Task||Subj) , data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim||Subj) , data = comp, family = "binomial"))
summaryBy(Acc ~ Task + Stim, data = comp)
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj) , data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1|Subj) , data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Task|Subj) , data = comp, family = "binomial"))
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj) , data = comp, family = "binomial"))
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data[4]$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Disjunction"#
	#print(summary(d))#
	d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "One Word"#
	d[d$Cond %in% c("Match-Adj", "Match-Noun","Match-Color","Mismatch-Disjunc","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
	d$Match <- "Match"#
	d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun","Mismatch-Disjunc"),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.2), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1b")#
print(catch)#
comp <- Comp_Import("./Exp1b")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Prep for w-subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "Disjunction"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "Disjunction"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA#
#GLMER analysis (Task removed for convergence)#
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
contrasts(comp$Stim)
contrasts(comp$Task)
library(lme4)#
catch <- Catch_Import("./Exp1b")#
print(catch)#
comp <- Comp_Import("./Exp1b")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
contrasts(comp$Task) <- c(-0.5,0.5)#
# Prep for w-subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
ezANOVA(subset(comp, Acc ==1 & Match == "Match"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Color"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "FixedColor"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#LMER Analyis (Type between Subj, Stim within)#
summary(glmer(Acc ~ Type * Stim + (1+Stim|Subj), data = comp, family = "binomial"))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
#Reaction Times#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big" & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark" & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Big" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type == "Dark" & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Accuracy#
ezANOVA(comp.omni, Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
# GLMER Analysis (Type between subject, stim removed for model convergence)#
summary(glmer(Acc ~ Type * Stim + (1|Subj), data = comp.omni, family = "binomial"))
comp.omni$Num.Stim <- 0#
comp.omni[comp.omni$Stim == "One Word",]$Num.Stim <- -1#
comp.omni[comp.omni$Stim == "Three Words",]$Num.Stim <- 1#
#
# GLMER Analysis (Type between subject, stim removed for model convergence)#
summary(glmer(Acc ~ Type * Num.Stim + (1|Subj), data = comp.omni, family = "binomial"))
comp.omni$Num.Stim <- 0#
comp.omni[comp.omni$Stim == "One Word",]$Num.Stim <- -1#
comp.omni[comp.omni$Stim == "Three Words",]$Num.Stim <- 1#
#
# GLMER Analysis (Type between subject, stim removed for model convergence)#
summary(glmer(Acc ~ Type * Num.Stim + (1+Num.Stim|Subj), data = comp.omni, family = "binomial"))
comp$Num.Stim <- 0#
comp[comp$Stim == "One Word",]$Num.Stim <- -1#
comp[comp$Stim == "Three Words",]$Num.Stim <- 1#
summary(glmer(Acc ~ Type * Num.Stim + (1|Subj), data = comp, family = "binomial"))
summary(comp)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d[d$Length == "3",]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Striped"#
	d[grep("spotted",d$stims$Pic, ignore.case = TRUE),]$PicType <- "Spotted"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block = rep(c(1,2), each = 150)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Block = d$Block, File = d$stimulus)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.5), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp3")#
print(catch)#
comp <- Comp_Import("./Exp3")#
comp <- subset(comp, Type %in% c("Adj3","Adj4", "Adv4"))#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
# Reaction Times#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4") & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj4"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adv4"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" & Stim != "Three Words" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj4" & Stim != "Three Words" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adv4" & Stim != "Three Words" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj3" & Stim != "One Word" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adj4" & Stim != "One Word" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type == "Adv4" & Stim != "One Word" ), rt, wid = .(Subj), within = .(Stim))$ANOVA#
#
# Accuracy#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4")), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4") & Stim != "One Word"), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Two Words"), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Three Words"), Acc, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
#
# GLMER Accuracy analysis: Doesn't converge with random slope for stim (detailedType between subject)#
comp$Num.Stim <- 0#
comp[comp$Stim == "One Word",]$Num.Stim <- -1#
comp[comp$Stim == "Three Words",]$Num.Stim <- 1#
summary(glmer(Acc ~ Type * Num.Stim + (1|Subj), data = comp, family = "binomial"))
summary(glmer(Acc ~ Type * Num.Stim + (1+Num.Stim|Subj), data = comp, family = "binomial"))
